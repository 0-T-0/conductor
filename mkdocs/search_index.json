{
    "docs": [
        {
            "location": "/", 
            "text": "Conductor is an \norchestration\n engine that runs in the cloud.  \n\n\nMotivation\n\n\nWe built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:\n\n\n\n\nAllow creating complex process / business flows in which individual task is implemented by a microservice.\n\n\nA JSON DSL based blueprint defines the execution flow.\n\n\nProvide visibility and traceability into the these process flows.\n\n\nExpose control semantics around pause, resume, restart, etc allowing for better devops experience.\n\n\nAllow greater reuse of existing microservices providing an easier path for onboarding.\n\n\nUser interface to visualize the process flows.\n\n\nAbility to synchronously process all the tasks when needed.\n\n\nAbility to scale millions of concurrently running process flows.\n\n\nBacked by a queuing service abstracted from the clients.\n\n\nBe able to operate on HTTP or other transports e.g. gRPC.\n\n\n\n\nWhy not peer to peer choreography?\n\n\nWith peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:\n\n\n\n\nProcess flows are \u201cembedded\u201d within the code of multiple application.\n\n\nOften, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.\n\n\nAlmost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#motivation", 
            "text": "We built Conductor to help us orchestrate microservices based process flows at Netflix with the following features:   Allow creating complex process / business flows in which individual task is implemented by a microservice.  A JSON DSL based blueprint defines the execution flow.  Provide visibility and traceability into the these process flows.  Expose control semantics around pause, resume, restart, etc allowing for better devops experience.  Allow greater reuse of existing microservices providing an easier path for onboarding.  User interface to visualize the process flows.  Ability to synchronously process all the tasks when needed.  Ability to scale millions of concurrently running process flows.  Backed by a queuing service abstracted from the clients.  Be able to operate on HTTP or other transports e.g. gRPC.   Why not peer to peer choreography?  With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities.\nPub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:   Process flows are \u201cembedded\u201d within the code of multiple application.  Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs.  Almost no way to systematically answer \u201chow much are we done with process X\u201d?", 
            "title": "Motivation"
        }, 
        {
            "location": "/intro/", 
            "text": "High Level Architecture\n\n\n\n\nThe API and stroage layers are pluggable and provides ability to work with different backend and queue service providers.\n\n\nInstalling and Running\n\n\nRequirements\n\n\n\n\nDatabase\n: Dynomite\n\n\nServlet Container\n: Tomcat, Jetty, or similar running JDK 1.8 or higher\n\n\n\n\nRun In-Memory Server\n\n\nFollow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor\n\n\nCheckout the source from github\n\n\ngit clone git@github.com:Netflix/conductor.git\n\n\n\n\nStart Local Server\n\n\ncd test-harness\n../gradlew server\n# wait for the server to come online\n\n\n\n\nSwagger APIs can be accessed at \nhttp://localhost:8080/swagger-ui/\n\n\nStart UI Server\n\n\ncd ui\ngulp watch\n\n\n\n\nLaunch UI at \nhttp://localhost:3000/\n\n\nRuntime Model\n\n\nConductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.\n\n\n\n\nNotes\n\n\n\n\nWorkers are remote systems and communicates ove HTTP (or any supported RPC mechanism) with conductor servers.\n\n\nTask Queues are used to schedule tasks for workers.  We use \ndyno-queues\n internally but it can easily be swapped with SQS or similar pub-sub mechanism.\n\n\nconductor-redis-persistence module uses \ndynomite\n for storaing the state and metadata along with [elastic[[3] for indexing backend.\n\n\nSee section under extending backend for implementing support for different databases for storage and indexing.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/intro/#high-level-architecture", 
            "text": "The API and stroage layers are pluggable and provides ability to work with different backend and queue service providers.", 
            "title": "High Level Architecture"
        }, 
        {
            "location": "/intro/#installing-and-running", 
            "text": "Requirements   Database : Dynomite  Servlet Container : Tomcat, Jetty, or similar running JDK 1.8 or higher", 
            "title": "Installing and Running"
        }, 
        {
            "location": "/intro/#run-in-memory-server", 
            "text": "Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor  Checkout the source from github  git clone git@github.com:Netflix/conductor.git  Start Local Server  cd test-harness\n../gradlew server\n# wait for the server to come online  Swagger APIs can be accessed at  http://localhost:8080/swagger-ui/  Start UI Server  cd ui\ngulp watch  Launch UI at  http://localhost:3000/", 
            "title": "Run In-Memory Server"
        }, 
        {
            "location": "/intro/#runtime-model", 
            "text": "Conductor follows RPC based communication model where workers are running on a separate machine from the server.  Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues.   Notes   Workers are remote systems and communicates ove HTTP (or any supported RPC mechanism) with conductor servers.  Task Queues are used to schedule tasks for workers.  We use  dyno-queues  internally but it can easily be swapped with SQS or similar pub-sub mechanism.  conductor-redis-persistence module uses  dynomite  for storaing the state and metadata along with [elastic[[3] for indexing backend.  See section under extending backend for implementing support for different databases for storage and indexing.", 
            "title": "Runtime Model"
        }, 
        {
            "location": "/intro/concepts/", 
            "text": "Workflow Definition\n\n\nWorkflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine.\n\nmore details\n\n\nTask Definition\n\n\n\n\nAll tasks need to be registered before they can be used by active workflows.\n\n\nA task can be re-used within multiple workflows.\n\nmore details", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/intro/concepts/#workflow-definition", 
            "text": "Workflows are defined using a JSON based DSL and includes a set of tasks that are executed as part of the workflows.  The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. more details", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/intro/concepts/#task-definition", 
            "text": "All tasks need to be registered before they can be used by active workflows.  A task can be re-used within multiple workflows. more details", 
            "title": "Task Definition"
        }, 
        {
            "location": "/intro/steps/", 
            "text": "Steps required for a new workflow to be registered and get executed:\n\n\n\n\nDefine task definitions used by the workflow.\n\n\nCreate the workflow definition\n\n\n\n\nCreate task worker(s) that polls for scheduled tasks at regular interval\n\n\nGET /tasks/poll/batch/{taskType}\n\n\nUpdate task status:\n\nPOST /tasks\n{\n    ... //json payload with task status\n}\n\n\n\n\n\n\nUse the REST endpoint to trigger the workflow execution\n\n\nPOST /workflow/{name}\n{\n    ... //json payload as workflow input\n}", 
            "title": "Highlevel Steps"
        }, 
        {
            "location": "/metadata/", 
            "text": "Workflow Definition\n\n\nA typical workflow has the following structure:\n\n\n{\n  \nname\n: \nworkflow_name\n,\n  \ndescription\n: \nDescription of workflow\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \nname_of_task\n,\n      \ntaskReferenceName\n: \nref_name_unique_within_blueprint\n,\n      \ninputParameters\n: {\n        \nmovieId\n: \n${workflow.input.movieId}\n,\n        \nurl\n: \n${workflow.input.fileLocation}\n\n      },\n      \ntype\n: \nSIMPLE\n,\n      ... (any other task specific parameters)\n    },\n    {}\n    ...\n  ],\n  \noutputParameters\n: {\n    \nencoded_url\n: \n${encode.output.location}\n\n  }\n}\n\n\n\n\nA sample encoding flow\n\n\n{\n  \nname\n: \nworkflow_name\n,\n  \ndescription\n: \nDescription of workflow\n,\n  \nversion\n: 1,\n  \ntasks\n: [\n    {\n      \nname\n: \nstore_in_s3\n,\n      \ntaskReferenceName\n: \nt1\n,\n      \ninputParameters\n: {\n        \nmovieId\n: \n${workflow.input.movieId}\n,\n        \nurl\n: \n${workflow.input.fileLocation}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    },\n    {\n      \nname\n: \nencode\n,\n      \ntaskReferenceName\n: \nencode\n,\n      \ninputParameters\n: {\n        \nrecipe\n: \n${workflow.input.recipe}\n,\n        \nlocation\n: \n${t1.output.s3Location}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    }\n  ],\n  \noutputParameters\n: {\n    \nencoded_url\n: \n${encode.output.location}\n\n  }\n}\n\n\n\n\nIn the above definition, tasks are series of steps that are executed in sequence.\n\n\nBelow is the list of all the parameters used to define a workflow blueprint:\n\n\n{\n    \nname\n: \nWorkflow Name. Used for all the operations\n,\n    \ndescription\n: \nHuman readable description\n,\n    \nversion\n: 1 //defines the version of workflow blueprint. Useful for versioning\n    \ninputParameters\n: \nList of required inputs to workflow\n,\n    \noutputParameters\n: \nMap of key - value used to determine the output of the workflow\n,\n    \nfailureWorkflow\n: \nName of another workflow to be started if this workflow fails\n,\n    \ntasks\n: \nlist of tasks that makes up this blueprint\n\n}\n\n\n\n\nTask Definition\n\n\nWorkflow tasks are defined within the \ntasks\n block in the blueprint.\n\nEach task represent either a system task (executed by the engine and used for control flow) or a user task which is executed on a remote system and communicates with the conductor server using REST APIs.\n\n\nTasks are defined using a JSON DSL.  Conductor maintain a registry of all the tasks.\n\n\n{\n  \nname\n: \nencode_task\n,\n  \nretryCount\n: 3,\n  \ntimeoutSeconds\n: 1200,\n  \ninputKeys\n: [\n    \nsourceRequestId\n,\n    \nqcElementType\n\n  ],\n  \noutputKeys\n: [\n    \nstate\n,\n    \nskipped\n,\n    \nresult\n\n  ],\n  \ntimeoutPolicy\n: \nTIME_OUT_WF\n,\n  \nretryLogic\n: \nFIXED\n,\n  \nretryDelaySeconds\n: 600,\n  \nresponseTimeoutSeconds\n: 3600\n}\n\n\n\n\n\n\nretryCount\n No. of retries to attempt when a task is marked as failure.\n\n\ntimeoutPolicy\n Can be either of the following:\n\n\nRETRY : Retries the task again\n\n\nTIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated\n\n\nALERT_ONLY : Registers a counter (task_timeout)\n\n\n\n\n\n\nretryLogic\n\n\nFIXED : Reschedule the task afer the \nretryDelaySeconds\n\n\nEXPONENTIAL_BACKOFF : reschedule after \nretryDelaySeconds  * attempNo\n\n\n\n\n\n\nresponseTimeoutSeconds\n if greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.\n\n\ninputKeys\n Set of inputs that task expects\n\n\noutputKeys\n SEt of output that task produces upon execution", 
            "title": "Definitions"
        }, 
        {
            "location": "/metadata/#workflow-definition", 
            "text": "A typical workflow has the following structure:  {\n   name :  workflow_name ,\n   description :  Description of workflow ,\n   version : 1,\n   tasks : [\n    {\n       name :  name_of_task ,\n       taskReferenceName :  ref_name_unique_within_blueprint ,\n       inputParameters : {\n         movieId :  ${workflow.input.movieId} ,\n         url :  ${workflow.input.fileLocation} \n      },\n       type :  SIMPLE ,\n      ... (any other task specific parameters)\n    },\n    {}\n    ...\n  ],\n   outputParameters : {\n     encoded_url :  ${encode.output.location} \n  }\n}  A sample encoding flow  {\n   name :  workflow_name ,\n   description :  Description of workflow ,\n   version : 1,\n   tasks : [\n    {\n       name :  store_in_s3 ,\n       taskReferenceName :  t1 ,\n       inputParameters : {\n         movieId :  ${workflow.input.movieId} ,\n         url :  ${workflow.input.fileLocation} \n      },\n       type :  SIMPLE \n    },\n    {\n       name :  encode ,\n       taskReferenceName :  encode ,\n       inputParameters : {\n         recipe :  ${workflow.input.recipe} ,\n         location :  ${t1.output.s3Location} \n      },\n       type :  SIMPLE \n    }\n  ],\n   outputParameters : {\n     encoded_url :  ${encode.output.location} \n  }\n}  In the above definition, tasks are series of steps that are executed in sequence.  Below is the list of all the parameters used to define a workflow blueprint:  {\n     name :  Workflow Name. Used for all the operations ,\n     description :  Human readable description ,\n     version : 1 //defines the version of workflow blueprint. Useful for versioning\n     inputParameters :  List of required inputs to workflow ,\n     outputParameters :  Map of key - value used to determine the output of the workflow ,\n     failureWorkflow :  Name of another workflow to be started if this workflow fails ,\n     tasks :  list of tasks that makes up this blueprint \n}", 
            "title": "Workflow Definition"
        }, 
        {
            "location": "/metadata/#task-definition", 
            "text": "Workflow tasks are defined within the  tasks  block in the blueprint. \nEach task represent either a system task (executed by the engine and used for control flow) or a user task which is executed on a remote system and communicates with the conductor server using REST APIs.  Tasks are defined using a JSON DSL.  Conductor maintain a registry of all the tasks.  {\n   name :  encode_task ,\n   retryCount : 3,\n   timeoutSeconds : 1200,\n   inputKeys : [\n     sourceRequestId ,\n     qcElementType \n  ],\n   outputKeys : [\n     state ,\n     skipped ,\n     result \n  ],\n   timeoutPolicy :  TIME_OUT_WF ,\n   retryLogic :  FIXED ,\n   retryDelaySeconds : 600,\n   responseTimeoutSeconds : 3600\n}   retryCount  No. of retries to attempt when a task is marked as failure.  timeoutPolicy  Can be either of the following:  RETRY : Retries the task again  TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated  ALERT_ONLY : Registers a counter (task_timeout)    retryLogic  FIXED : Reschedule the task afer the  retryDelaySeconds  EXPONENTIAL_BACKOFF : reschedule after  retryDelaySeconds  * attempNo    responseTimeoutSeconds  if greater than 0, the task is rescheduled if not updated with a status after this time.  Useful when the worker polls for the task but fails to complete due to errors/network failure.  inputKeys  Set of inputs that task expects  outputKeys  SEt of output that task produces upon execution", 
            "title": "Task Definition"
        }, 
        {
            "location": "/metadata/task/", 
            "text": "Workflow Tasks are of two types:\n\n\n\n\nSystem tasks (e.g. DECISION, FORK, JOIN etc.)\n\n\nRemote worker tasks.\n\n\n\n\nSystem tasks are executed within the JVM of the engine's server.  User defined tasks are remote tasks that executes on remote JVM and talks to the conductor server using REST endpoints.\n\n\n\n\nSIMPLE\n Task that is executed by a worker running on a remote machine.  Orchestration engine schedules the task and awaits completion.\n\n\nDECIDE\n Decision task used to create a branch.  Only one branch is executed based on the condition.\n\n\nFORK\n Forks a parallel set of tasks.  Each set is scheduled to be executed in parallel.\n\n\nJOIN\n Waits for one or more tasks to be completed before proceeding.  Used in conjunction with \nFORK\n or _ FORK_JOIN_DYNAMIC_\n\n\nSUB_WORKFLOW\n Allows nesting another workflow as a task.  When executed, system spawns the workflow and task waits its completion.  If the newly spawned workflow fails, the task is marked as FAILEd.\n\n\nDYNAMIC\n A dynamic task.  Similar to \nSIMPLE\n.  The difference being the name of the task to be scheduled is determined at runtime using parameters from workflow input or other task input/output.\n\n\nFORK_JOIN_DYNAMIC\n\n\n\n\nConductor provides an API to create user defined tasks that are excuted in the same JVM as the engine.  see \nom.netflix.workflow.cpe.core.execution.tasks.WorkflowSystemTask\n interface for details.", 
            "title": "Workflow Tasks"
        }, 
        {
            "location": "/metadata/io/", 
            "text": "Each task in the blueprint takes a set of inputs defined using \ninputParameters\n block.\nHere is a sample workflow task in a blueprint:\n\n\n   {\n      \nname\n: \nname_of_task\n,\n      \ntaskReferenceName\n: \nref_name_unique_within_blueprint\n,\n      \ninputParameters\n: {\n        \nmovieId\n: \n${workflow.input.movieId}\n,\n        \nurl\n: \n${workflow.input.fileLocation}\n\n      },\n      \ntype\n: \nSIMPLE\n\n    }\n\n\n\n\ninputParameters is a map with keys representing the task input.  The value of the key identifies the source of data at runtime.  \n\n\nThe syntax is as follows:\n\nsource.input|output.key\n\n\nThe source can be either \"workflow\" or a task reference name.\ne.g. \nencode_task.output.location\n will read the \nlocation\n key from the output of the task identified by reference name \nencode_task\n.\n\n\nString and numeric literals are wired directly as the JSON string or numeric literals.\ne.g.\n\n\n{\n \nsome_key\n: \nconst string value\n,\n \nnum_value: 42\n}", 
            "title": "Wiring inputs"
        }, 
        {
            "location": "/metadata/systask/", 
            "text": "Dynamic Task\n\n\nRequired paramters:\n\ndynamicTaskNameParam\n Name of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.\n\n\nExample Task configuration:\n\n\n{\n  \nname\n: \nuser_task\n,\n  \ntaskReferenceName\n: \nt1\n,\n  \ninputParameters\n: {\n    \nfiles\n: \n${workflow.input.files}\n,\n    \ntaskToExecute\n: \n${workflow.input.user_supplied_task}\n\n  },\n  \ntype\n: \nDYNAMIC\n,\n  \ndynamicTaskNameParam\n: \ntaskToExecute\n\n}\n\n\n\n\nDecision Task\n\n\nA decision task is similar to \ncase...switch\n statement in a programming langugage.\nThe task takes 3 parameters:\n\n\n\n\ncaseValueParam\n Name of the parameter in task input whose value will be used as a switch.\n\n\ndecisionCases\n Map where key is possible values of \ncaseValueParam\n with value being list of tasks to be executed.\n\n\ndefaultCase\n List of tasks to be executed when no matching value if found in decision case (default condition)\n\n\n\n\nSample\n\n\n{\n  \nname\n: \ndecide_task\n,\n  \ntaskReferenceName\n: \ndecide1\n,\n  \ninputParameters\n: {\n    \ncase_value_param\n: \n${workflow.input.movieType}\n\n  },\n  \ntype\n: \nDECISION\n,\n  \ncaseValueParam\n: \ncase_value_param\n,\n  \ndecisionCases\n: {\n    \nShow\n: [\n      {\n        \nname\n: \nsetup_episodes\n,\n        \ntaskReferenceName\n: \nse1\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_episode_artwork\n,\n        \ntaskReferenceName\n: \nga\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ],\n    \nMovie\n: [\n      {\n        \nname\n: \nsetup_movie\n,\n        \ntaskReferenceName\n: \nsm\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      },\n      {\n        \nname\n: \ngenerate_movie_artwork\n,\n        \ntaskReferenceName\n: \ngma\n,\n        \ninputParameters\n: {\n          \nmovieId\n: \n${workflow.input.movieId}\n\n        },\n        \ntype\n: \nSIMPLE\n\n      }\n    ]\n  }\n}\n\n\n\n\nFork\n\n\nFork is used to schedule parallel set of tasks.\n\n\nParameters\n\n\n\n\nforkTasks\n A list of list of tasks.  Each list if scheduled to be executed in parallel.  However, within the list tasks are scheduled in serial.\n\n\n\n\nSample\n\n\n{\n  \nforkTasks\n: [\n    [\n      {\n        \nname\n: \ntask11\n,\n        \ntaskReferenceName\n: \nt11\n\n      },\n      {\n        \nname\n: \ntask12\n,\n        \ntaskReferenceName\n: \nt12\n\n      }\n    ],\n    [\n      {\n        \nname\n: \ntask21\n,\n        \ntaskReferenceName\n: \nt21\n\n      },\n      {\n        \nname\n: \ntask22\n,\n        \ntaskReferenceName\n: \nt22\n\n      }\n    ]\n  ]\n}\n\n\n\n\nWhen executed, \ntask11\n and \ntask21\n are scheduled to be executed at the same time.\n\n\nJoin\n\n\nJoin task is used to wait for completion of one or more tasks spawned by fork tasks.\n\n\nParameters\n\n\njoinOn\n List of task reference name, for which the JOIN will wait for completion.\n\n\nSample\n\n\n{\n    \njoinOn\n: [\ntaskRef1\n, \ntaskRef3\n]\n}\n\n\n\n\nDynamic Fork\n\n\nA dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.\n\n\nSub Workflow", 
            "title": "System Tasks"
        }, 
        {
            "location": "/metadata/systask/#dynamic-task", 
            "text": "Required paramters: dynamicTaskNameParam  Name of the parameter from the task input whose value is used to schedule the task.  e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.  Example Task configuration:  {\n   name :  user_task ,\n   taskReferenceName :  t1 ,\n   inputParameters : {\n     files :  ${workflow.input.files} ,\n     taskToExecute :  ${workflow.input.user_supplied_task} \n  },\n   type :  DYNAMIC ,\n   dynamicTaskNameParam :  taskToExecute \n}", 
            "title": "Dynamic Task"
        }, 
        {
            "location": "/metadata/systask/#decision-task", 
            "text": "A decision task is similar to  case...switch  statement in a programming langugage.\nThe task takes 3 parameters:   caseValueParam  Name of the parameter in task input whose value will be used as a switch.  decisionCases  Map where key is possible values of  caseValueParam  with value being list of tasks to be executed.  defaultCase  List of tasks to be executed when no matching value if found in decision case (default condition)   Sample  {\n   name :  decide_task ,\n   taskReferenceName :  decide1 ,\n   inputParameters : {\n     case_value_param :  ${workflow.input.movieType} \n  },\n   type :  DECISION ,\n   caseValueParam :  case_value_param ,\n   decisionCases : {\n     Show : [\n      {\n         name :  setup_episodes ,\n         taskReferenceName :  se1 ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_episode_artwork ,\n         taskReferenceName :  ga ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ],\n     Movie : [\n      {\n         name :  setup_movie ,\n         taskReferenceName :  sm ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      },\n      {\n         name :  generate_movie_artwork ,\n         taskReferenceName :  gma ,\n         inputParameters : {\n           movieId :  ${workflow.input.movieId} \n        },\n         type :  SIMPLE \n      }\n    ]\n  }\n}", 
            "title": "Decision Task"
        }, 
        {
            "location": "/metadata/systask/#fork", 
            "text": "Fork is used to schedule parallel set of tasks.  Parameters   forkTasks  A list of list of tasks.  Each list if scheduled to be executed in parallel.  However, within the list tasks are scheduled in serial.   Sample  {\n   forkTasks : [\n    [\n      {\n         name :  task11 ,\n         taskReferenceName :  t11 \n      },\n      {\n         name :  task12 ,\n         taskReferenceName :  t12 \n      }\n    ],\n    [\n      {\n         name :  task21 ,\n         taskReferenceName :  t21 \n      },\n      {\n         name :  task22 ,\n         taskReferenceName :  t22 \n      }\n    ]\n  ]\n}  When executed,  task11  and  task21  are scheduled to be executed at the same time.", 
            "title": "Fork"
        }, 
        {
            "location": "/metadata/systask/#join", 
            "text": "Join task is used to wait for completion of one or more tasks spawned by fork tasks.  Parameters  joinOn  List of task reference name, for which the JOIN will wait for completion.  Sample  {\n     joinOn : [ taskRef1 ,  taskRef3 ]\n}", 
            "title": "Join"
        }, 
        {
            "location": "/metadata/systask/#dynamic-fork", 
            "text": "A dynamic fork is same as FORK_JOIN task.  Except that the list of tasks to be forked is provided at runtime using task's input.  Useful when number of tasks to be forked is not fixed and varies based on the input.", 
            "title": "Dynamic Fork"
        }, 
        {
            "location": "/metadata/systask/#sub-workflow", 
            "text": "", 
            "title": "Sub Workflow"
        }, 
        {
            "location": "/extend/tasks/", 
            "text": "Contrib module provides additional functionality and task that are executed on the workflow server.\n\n\nWait Task\n\n\nA wait task is implemented as a gate that remains in \nIN_PROGRESS\n state unless marked as \nCOMPLETED\n or \nFAILED\n by an external trigger.\nTo use a wait task, set the task type as \nWAIT\n\n\nExernal Triggers for Wait Task\n\n\nTask Resource endpoint can be used to update the status of a task to a terminate state. \n\n\nContrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as \nCOMPLETED\n or \nFAILED\n.  \n\n\nSQS Queues\n\n\n\n\n\n\nSQS queues used by the server to update the task status can be retrieve using the following URL:\n\n\nGET /queue\n\n\n\n\n\n\nWhen updating the status of the task, the message needs to conform to the following spec:\n\n\n\n\nMessage has to be a valid JSON string.\n\n\nThe message JSON should contain a key named \nexternalId\n with the value being a JSONified string that contains the following keys:\n\n\nworkflowId\n: Id of the workflow\n\n\ntaskRefName\n: Task reference name that should be updated.\n\n\n\n\n\n\nEach queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a \nCOMPLETED\n queue marks the task status as \nCOMPLETED\n.\n\n\nTasks' output is updated with the message.\n\n\n\n\n\n\n\n\nExample:\n\n\n{\n  \"some_key\": \"valuex\",\n  \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\"\n}\n\n\n\n\n\n\nHTTP\n\n\nAn HTTP task is used to make calls to another microservice over HTTP.\nThe task expects an input parameter named \nhttp_request\n as part of the task's input with the following details:\n\n\nuri: URI for the service.  Can be a partial when using vipAddress or includes the server address.\n\nmethod: HTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD\n\naccept: Accpet header as required by server.\n\ncontentType: Content Type - supported types are text/plain, text/html and, application/json\n\nheaders: A map of additional http headers to be sent along with the request.\n\nbody:  Request body.\n\nvipAddress: When using discovery based service URLs. A custom \n\n\n\n\nExamples\n\n\nTask Input payload using vipAddress\n\n\n{\n  \nhttp_request\n: {\n    \nvipAddress\n: \nexamplevip-prod\n,\n    \nuri\n: \n/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nTask Input using an absolute URL\n\n\n{\n  \nhttp_request\n: {\n    \nuri\n: \nhttp://example.com/\n,\n    \nmethod\n: \nGET\n,\n    \naccept\n: \ntext/plain\n\n  }\n}\n\n\n\n\nThe task is marked as \nFAILED\n if the request cannot be completed or the remote server returns non successful status code.", 
            "title": "Extended Tasks & Features"
        }, 
        {
            "location": "/extend/tasks/#wait-task", 
            "text": "A wait task is implemented as a gate that remains in  IN_PROGRESS  state unless marked as  COMPLETED  or  FAILED  by an external trigger.\nTo use a wait task, set the task type as  WAIT", 
            "title": "Wait Task"
        }, 
        {
            "location": "/extend/tasks/#exernal-triggers-for-wait-task", 
            "text": "Task Resource endpoint can be used to update the status of a task to a terminate state.   Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on.  As the messages arrive, they are marked as  COMPLETED  or  FAILED .", 
            "title": "Exernal Triggers for Wait Task"
        }, 
        {
            "location": "/extend/tasks/#sqs-queues", 
            "text": "SQS queues used by the server to update the task status can be retrieve using the following URL:  GET /queue    When updating the status of the task, the message needs to conform to the following spec:   Message has to be a valid JSON string.  The message JSON should contain a key named  externalId  with the value being a JSONified string that contains the following keys:  workflowId : Id of the workflow  taskRefName : Task reference name that should be updated.    Each queue represents a specific task status and tasks are marked accordingly.  e.g. message coming to a  COMPLETED  queue marks the task status as  COMPLETED .  Tasks' output is updated with the message.     Example:  {\n  \"some_key\": \"valuex\",\n  \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\"\n}", 
            "title": "SQS Queues"
        }, 
        {
            "location": "/extend/tasks/#http", 
            "text": "An HTTP task is used to make calls to another microservice over HTTP.\nThe task expects an input parameter named  http_request  as part of the task's input with the following details:  uri: URI for the service.  Can be a partial when using vipAddress or includes the server address.\n\nmethod: HTTP method.  One of the GET, PUT, POST, DELETE, OPTIONS, HEAD\n\naccept: Accpet header as required by server.\n\ncontentType: Content Type - supported types are text/plain, text/html and, application/json\n\nheaders: A map of additional http headers to be sent along with the request.\n\nbody:  Request body.\n\nvipAddress: When using discovery based service URLs. A custom   Examples  Task Input payload using vipAddress  {\n   http_request : {\n     vipAddress :  examplevip-prod ,\n     uri :  / ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  Task Input using an absolute URL  {\n   http_request : {\n     uri :  http://example.com/ ,\n     method :  GET ,\n     accept :  text/plain \n  }\n}  The task is marked as  FAILED  if the request cannot be completed or the remote server returns non successful status code.", 
            "title": "HTTP"
        }, 
        {
            "location": "/runtime/", 
            "text": "Metadata\n\n\nWorkflow blueprints are managed via \n/metadata\n resource endpoints.\n\n\nInstances\n\n\nEach running workflow instance is identified by a unique instance id.  This id is used for managing the lifecycle of workflow instance.  The following operations are possible:\n\n\n\n\nStart a workflow\n\n\nTerminate\n\n\nRestart\n\n\nPause\n\n\nResume\n\n\nRerun\n\n\nSearch for workflows", 
            "title": "Workflow Management"
        }, 
        {
            "location": "/runtime/#metadata", 
            "text": "Workflow blueprints are managed via  /metadata  resource endpoints.", 
            "title": "Metadata"
        }, 
        {
            "location": "/runtime/#instances", 
            "text": "Each running workflow instance is identified by a unique instance id.  This id is used for managing the lifecycle of workflow instance.  The following operations are possible:   Start a workflow  Terminate  Restart  Pause  Resume  Rerun  Search for workflows", 
            "title": "Instances"
        }, 
        {
            "location": "/extend/", 
            "text": "System Tasks\n\n\nTo create system tasks follow the steps below:\n\n\n\n\nExtend \ncom.netflix.workflow.cpe.core.execution.tasks.WorkflowSystemTask\n\n\nInstantiate the new classs as part of the statup (eager singleton)\n\n\n\n\nBackend\n\n\nConductor provides a pluggable backend.  The current implementation uses Dynomite.\n\n\nThere are 4 interfaces that needs to be implemented for each backend:\n\n\n//Store for workfow and task definitions\ncom.netflix.workflow.cpe.dao.MetadataDAO\n\n\n\n\n//Store for workflow executions\ncom.netflix.workflow.cpe.dao.ExecutionDAO\n\n\n\n\n//Index for workflow executions\ncom.netflix.workflow.cpe.dao.IndexDAO\n\n\n\n\n//Queue provider for tasks\ncom.netflix.workflow.cpe.dao.QueueDAO\n\n\n\n\nIt is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.", 
            "title": "Extending Features"
        }, 
        {
            "location": "/extend/#system-tasks", 
            "text": "To create system tasks follow the steps below:   Extend  com.netflix.workflow.cpe.core.execution.tasks.WorkflowSystemTask  Instantiate the new classs as part of the statup (eager singleton)", 
            "title": "System Tasks"
        }, 
        {
            "location": "/extend/#backend", 
            "text": "Conductor provides a pluggable backend.  The current implementation uses Dynomite.  There are 4 interfaces that needs to be implemented for each backend:  //Store for workfow and task definitions\ncom.netflix.workflow.cpe.dao.MetadataDAO  //Store for workflow executions\ncom.netflix.workflow.cpe.dao.ExecutionDAO  //Index for workflow executions\ncom.netflix.workflow.cpe.dao.IndexDAO  //Queue provider for tasks\ncom.netflix.workflow.cpe.dao.QueueDAO  It is possible to mix and match different implementation for each of these.\ne.g. SQS for queueing and a relational store for others.", 
            "title": "Backend"
        }, 
        {
            "location": "/metrics/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\nworkflow_server_error\n\n\nRate at which server side error is happening\n\n\nmethodName\n\n\n\n\n\n\nworkflow_failure\n\n\nCounter for failing workflows\n\n\nworkflowName, status\n\n\n\n\n\n\nworkflow_start_error\n\n\nCounter for failing to start a workflow\n\n\nworkflowName\n\n\n\n\n\n\nworkflow_running\n\n\nCounter for no. of running workflows\n\n\nworkflowName, version\n\n\n\n\n\n\ntask_queue_wait\n\n\nTime spent by a task in queue\n\n\ntaskType\n\n\n\n\n\n\ntask_execution\n\n\nTime taken to execute a task\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_poll\n\n\nTime taken to poll for a task\n\n\ntaskType\n\n\n\n\n\n\ntask_queue_depth\n\n\nPending tasks queue depth\n\n\ntaskType\n\n\n\n\n\n\ntask_timeout\n\n\nCounter for timed out tasks\n\n\ntaskType", 
            "title": "Server Metrics"
        }, 
        {
            "location": "/metrics/client/", 
            "text": "Conductor uses \nspectator\n to collect the metrics.\n\n\nWhen using the Java client, the following metrics are published:\n\n\n\n\n\n\n\n\nName\n\n\nPurpose\n\n\nTags\n\n\n\n\n\n\n\n\n\n\ntask_execution_queue_full\n\n\nCounter to record execution queue has saturated\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_error\n\n\nClient error when polling for a task queue\n\n\ntaskType, includeRetries, status\n\n\n\n\n\n\ntask_execute_error\n\n\nExcution error\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_failed\n\n\nTask ack failed\n\n\ntaskType\n\n\n\n\n\n\ntask_ack_error\n\n\nTask ack has encountered an exception\n\n\ntaskType\n\n\n\n\n\n\ntask_update_error\n\n\nTask status cannot be updated back to server\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_counter\n\n\nIncremented each time polling is done\n\n\ntaskType\n\n\n\n\n\n\ntask_poll_time\n\n\nTime to poll for a batch of tasks\n\n\ntaskType\n\n\n\n\n\n\ntask_execute_time\n\n\nTime to execute a task\n\n\ntaskType\n\n\n\n\n\n\n\n\nMetrics on client side supplements the one collected from server in identifying the network as well as client side issues.", 
            "title": "Worker Metrics"
        }, 
        {
            "location": "/license/", 
            "text": "Copyright 2016 Netflix, Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "License"
        }
    ]
}